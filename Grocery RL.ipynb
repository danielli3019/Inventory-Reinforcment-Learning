{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5320a9-50b7-4c1a-ad40-08318caeeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda5198f-6c86-440e-9f7d-40ca2bf9197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(\"keras:\", keras.__version__)\n",
    "\n",
    "# for building DQN model\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406d08c5-9048-45da-a551-ca1b310188d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_32524\\2441484184.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CompetitionDays'][df['CompetitionDays'] < 0] = 0\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_32524\\2441484184.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Promo2'][df['Promo2Days'] < 0] = False\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_32524\\2441484184.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Promo2Days'][df['Promo2Days'] < 0] = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Promo</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016179</th>\n",
       "      <td>85</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016353</th>\n",
       "      <td>259</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016356</th>\n",
       "      <td>262</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016368</th>\n",
       "      <td>274</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016429</th>\n",
       "      <td>335</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Store       Date  DayOfWeek  Promo  Sales\n",
       "1016179     85 2013-01-01          2      0   4220\n",
       "1016353    259 2013-01-01          2      0   6851\n",
       "1016356    262 2013-01-01          2      0  17267\n",
       "1016368    274 2013-01-01          2      0   3102\n",
       "1016429    335 2013-01-01          2      0   2401"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Pre-processing\n",
    "sales_df = pd.read_csv(\"train.csv\",dtype={'StateHoliday': 'S'})\n",
    "store_df = pd.read_csv(\"store.csv\")\n",
    "store_df[['CompetitionDistance','CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2SinceYear','Promo2SinceWeek']] = store_df[['CompetitionDistance','CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2SinceYear','Promo2SinceWeek']].astype('Int64', errors='ignore')\n",
    "store_df['CompetitionOpenSinceMonth'] = store_df['CompetitionOpenSinceMonth'].fillna(12)\n",
    "store_df['CompetitionOpenSinceYear'] = store_df['CompetitionOpenSinceYear'].fillna(2099)\n",
    "store_df['Promo2SinceWeek'] = store_df['Promo2SinceWeek'].fillna(51)\n",
    "store_df['Promo2SinceYear'] = store_df['Promo2SinceYear'].fillna(2099)\n",
    "df =  pd.merge(sales_df, store_df, how=\"left\", on=[\"Store\"])\n",
    "df['CompetitionDistance'] = df['CompetitionDistance'].fillna(0)\n",
    "df['CompetitiondateInt']=df['CompetitionOpenSinceYear'].astype(str) + df['CompetitionOpenSinceMonth'].astype(str).str.zfill(2)+ \"01\"\n",
    "df['CompetitionDate'] = pd.to_datetime(df['CompetitiondateInt'], format='%Y%m%d')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df['CompetitionDays'] = (df['Date'] - df['CompetitionDate']) / np.timedelta64(1, 'D')\n",
    "df['CompetitionDays'][df['CompetitionDays'] < 0] = 0\n",
    "df['Promo2DateInt'] = df.Promo2SinceYear*100+df.Promo2SinceWeek\n",
    "df['Promo2Date'] = pd.to_datetime(df['Promo2DateInt'].astype(str) + '0', format='%Y%W%w')\n",
    "df['Promo2Days'] = (df['Date'] - df['Promo2Date']) / np.timedelta64(1, 'D')\n",
    "df.PromoInterval1 = pd.to_datetime(df.PromoInterval1, format='%b').dt.month\n",
    "df.PromoInterval2 = pd.to_datetime(df.PromoInterval2, format='%b').dt.month\n",
    "df.PromoInterval3 = pd.to_datetime(df.PromoInterval3, format='%b').dt.month\n",
    "df.PromoInterval4 = pd.to_datetime(df.PromoInterval4, format='%b').dt.month\n",
    "df['Promo2'] = np.logical_or(df['Date'].dt.month==df.PromoInterval1 , np.logical_or(df['Date'].dt.month==df.PromoInterval2 , np.logical_or(df['Date'].dt.month==df.PromoInterval3 , df['Date'].dt.month==df.PromoInterval4)))\n",
    "df['Promo2'][df['Promo2Days'] < 0] = False\n",
    "df['Promo2Days'][df['Promo2Days'] < 0] = 0\n",
    "df['StateHoliday'] = pd.factorize(df['StateHoliday'])[0]\n",
    "df['StoreType'] = pd.factorize(df['StoreType'])[0] + 1\n",
    "df['Assortment'] = pd.factorize(df['Assortment'])[0] + 1\n",
    "df['Promo2'] = pd.factorize(df['Promo2'])[0]\n",
    "df = df [df['Open'] == 1]\n",
    "df = df[['Store','Date','DayOfWeek','Promo','Sales']]\n",
    "df = df.sort_values(by=['Date','Store'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60567440-1c90-4531-a88b-da7042dd4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grocery_store (gym.Env):\n",
    "    # possible actions\n",
    "    ACTION_LIST = [[0,0], [15000,0],[30000,0],[0,10000]]\n",
    "\n",
    "    # inventory capacity\n",
    "    INV_CAP = 40000\n",
    "\n",
    "    # Pre-order Delay\n",
    "    PO_DELAY = 2\n",
    "\n",
    "    # Margin & Cost\n",
    "    PO_MARGIN = 0.8\n",
    "    EO_MARGIN = 0.9\n",
    "    TRANS_COST = 500\n",
    "    TRANS_MARGIN = 0.03\n",
    "\n",
    "    # Inventory State Bin Size\n",
    "    INV_BIN = 10000\n",
    "    PRE_ORDER_TMR = 15000\n",
    "    PRE_ORDER_LTR = 15000\n",
    "    \n",
    "    # Delivery Schedule\n",
    "    Delivery_Sch = np.zeros(PO_DELAY)\n",
    "    \n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\"]\n",
    "        }\n",
    "\n",
    "\n",
    "    def __init__ (self, df, store_id, strt, end):\n",
    "        # ACTION SPACE\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([self.INV_CAP//self.INV_BIN+1, self.INV_CAP//self.PRE_ORDER_TMR+1] + \\\n",
    "        [self.INV_CAP*(self.PO_DELAY-1)//self.PRE_ORDER_LTR+1] + \\\n",
    "        [8,2])\n",
    "        # Passive environment\n",
    "        self.environment = df[df.Store==store_id][['DayOfWeek','Promo']].values\n",
    "        self.environment = self.environment[strt:end,:]\n",
    "        self.sales = df[df.Store==store_id]['Sales'].values\n",
    "        self.sales = self.sales[strt:end]\n",
    "        \n",
    "        # Initiate State Information\n",
    "        self.reset() \n",
    "\n",
    "    def reset (self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial passive environment and inventory.\n",
    "        Returns\n",
    "        -------\n",
    "        State as a tuple of all active and passive envrionment\n",
    "        \"\"\"\n",
    "        # Assuming inventory always start at zero\n",
    "        self.inventory = 0\n",
    "\n",
    "        # Delivery Schedule\n",
    "        self.Delivery_Sch = [0]*self.PO_DELAY\n",
    "        \n",
    "        # Reset passive environment to beginning\n",
    "        self.env_index = 0\n",
    "        \n",
    "        # MDP components\n",
    "        self.state = tuple([self.inventory//self.INV_BIN, self.Delivery_Sch[0]//self.PRE_ORDER_TMR] + \\\n",
    "        [sum(self.Delivery_Sch[1:])//self.PRE_ORDER_LTR] + \\\n",
    "        [self.environment[self.env_index][0],self.environment[self.env_index][1]])\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        # Assume all existing inventory at pre-order cost\n",
    "        self.cost = self.PO_MARGIN\n",
    "        # Overstock Cost and Order Cost\n",
    "        self.overstock_cost = 0\n",
    "        self.order_cost = 0\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        \"\"\"\n",
    "        The agent takes a step in the environment.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : Discrete\n",
    "        Returns\n",
    "        -------\n",
    "        state, reward, done, info : tuple\n",
    "            state (tuple) :\n",
    "                a tuple of all active and passive envrionment.\n",
    "            reward (float) :\n",
    "                immediate profit of the grocery store\n",
    "            done (bool) :\n",
    "                whether this is the last period in the time series\n",
    "            info (dict) :\n",
    "                 diagnostic information useful for debugging. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.env_index == len(self.environment) - 1:\n",
    "            self.done = True;\n",
    "\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 0:\n",
    "            self.order_cost = 0\n",
    "        else:\n",
    "            self.order_cost = self.TRANS_COST\n",
    "            \n",
    "        # Reward\n",
    "        # If there's no inventory, the store will be closed. Order cost/Transportation cost is assumed to be accrued at order date\n",
    "        self.reward = min(self.sales[self.env_index], self.inventory) * (1 - self.cost) - self.order_cost - self.overstock_cost\n",
    "        \n",
    "        order_delivered = self.Delivery_Sch.pop(0)     \n",
    "        \n",
    "        # Update cost\n",
    "        self.cost = (self.inventory * self.cost - min(self.sales[self.env_index], self.inventory) * self.cost +\\\n",
    "                     max(min(self.INV_CAP - (self.inventory - min(self.sales[self.env_index], self.inventory)) - order_delivered, self.ACTION_LIST[action][1]),0) * self.EO_MARGIN +\\\n",
    "                     max(min(self.INV_CAP - (self.inventory - min(self.sales[self.env_index], self.inventory)), order_delivered),0) * self.PO_MARGIN) \\\n",
    "        / max(min(self.inventory - min(self.sales[self.env_index], self.inventory) + self.ACTION_LIST[action][1] + order_delivered, self.INV_CAP), 0.01)\n",
    "      \n",
    "        # Update inventory for tomorrow\n",
    "        # Current Inventory - Item sold in the day + Emergency order + dequeue Pre-order delivered tomorrow\n",
    "        self.inventory = self.inventory - \\\n",
    "        min(self.sales[self.env_index], self.inventory) + \\\n",
    "        self.ACTION_LIST[action][1] + \\\n",
    "        order_delivered\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Update overstock cost\n",
    "        if self.inventory > self.INV_CAP:\n",
    "            self.overstock_cost = self.TRANS_COST #inventory exceed limit, excess inventory will be returned at same transportation cost on the next day\n",
    "            self.inventory = self.INV_CAP\n",
    "        else:\n",
    "            self.overstock_cost = 0\n",
    "                \n",
    "        # Update Delivery Queue\n",
    "        self.Delivery_Sch += [self.ACTION_LIST[action][0]]\n",
    "        \n",
    "        # State\n",
    "        self.state = tuple([self.inventory//self.INV_BIN, self.Delivery_Sch[0]//self.PRE_ORDER_TMR] + \\\n",
    "        [sum(self.Delivery_Sch[1:])//self.PRE_ORDER_LTR]+ \\\n",
    "        [self.environment[self.env_index][0],self.environment[self.env_index][1]])\n",
    "        \n",
    "        # Info \n",
    "        self.info = {\"Inventory\":self.inventory,\"Schedule\": self.Delivery_Sch,\"Emerg_Order\":self.ACTION_LIST[action][1],\"Sales\":self.sales[self.env_index],\"Cost\":self.cost}\n",
    "\n",
    "        # Update Index\n",
    "        self.env_index += 1\n",
    "        \n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)\n",
    "            \n",
    "        return [self.state, self.reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def render (self, mode=\"human\"):\n",
    "        s = \"position: {:2d}  reward: {:2d}  info: {}\"\n",
    "        print(s.format(self.state, self.reward, self.info))\n",
    "\n",
    "\n",
    "    def close (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083df0ec-6fd3-4bae-92fa-35ecdf720b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  4\n",
      "State Size:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 0],\n",
       "       [4, 0],\n",
       "       [5, 0],\n",
       "       ...,\n",
       "       [5, 0],\n",
       "       [6, 0],\n",
       "       [1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Grocery_store(df,1007,0,int(np.floor(len(df[df.Store==1007])*0.9)))\n",
    "# env.render()\n",
    "env_test = Grocery_store(df,1007,int(np.floor(len(df[df.Store==1007])*0.9)), len(df[df.Store==1007]))\n",
    "action_size = env.action_space.n\n",
    "print('Action Space: ', action_size)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('State Size: ', state_size)\n",
    "\n",
    "env.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af1846d-c6eb-4bbf-93ee-dba2b8779ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, dueling=False, DDQN=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dueling = dueling\n",
    "        self.DDQN = DDQN\n",
    "        # we keep adding to the list (i.e. replay memory used for training)\n",
    "        # as soon as we reach 2000, oldest elements are removed (i.e. just use last 2000 memories)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        ## begin: hyper parameters\n",
    "        self.gamma = 1 # discount\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        \n",
    "        self.epsilon_decay = 0.99 # exploration rate decay\n",
    "        self.epsilon_min = 0.01 # exploration rate minimum\n",
    "        \n",
    "        self.learning_rate = 0.0000001 # for SGD\n",
    "        ## end\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.model_t = self._build_model_t()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # NN model to approximate Q-values\n",
    "        model_input = Input(shape=(self.state_size,))\n",
    "        \n",
    "        layer = Dense(64, input_dim = self.state_size, activation='relu')(model_input)\n",
    "        layer = Dense(64, activation='relu')(layer)\n",
    "        if self.dueling:\n",
    "            state = Dense(1, activation='linear')(layer)\n",
    "            adv = Dense(self.action_size, activation='linear')(layer)\n",
    "            layer = state + (adv - tf.math.reduce_mean(adv, axis=1, keepdims=True))\n",
    "        else:\n",
    "            # Regular DQN Model\n",
    "            # linear activation: because we are directly modeling the actions \n",
    "            # (i.e. value of the actions, so a regression problem), probs etc not needed\n",
    "            layer = Dense(self.action_size, activation='linear')(layer)\n",
    "        model = Model(model_input, layer)\n",
    "        # mse is better choice for loss, e.g. mae performed terribly\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr = self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _build_model_t(self):\n",
    "        # NN model to approximate Q-values\n",
    "        model_input = Input(shape=(self.state_size,))\n",
    "        \n",
    "        layer = Dense(64, input_dim = self.state_size, activation='relu')(model_input)\n",
    "        layer = Dense(64, activation='relu')(layer)\n",
    "        \n",
    "        if self.dueling:\n",
    "            state = Dense(1, activation='linear')(layer)\n",
    "            adv = Dense(self.action_size, activation='linear')(layer)\n",
    "            layer = state + (adv - tf.math.reduce_mean(adv, axis=1, keepdims=True))\n",
    "        else:\n",
    "            # Regular DQN Model\n",
    "            # linear activation: because we are directly modeling the actions \n",
    "            # (i.e. value of the actions, so a regression problem), probs etc not needed\n",
    "            layer = Dense(self.action_size, activation='linear')(layer)\n",
    "        model = Model(model_input, layer)\n",
    "        # mse is better choice for loss, e.g. mae performed terribly\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # exploratory action (i.e. left or right)\n",
    "        # over time as epsilon decays, model will do more exploitation than exploration \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # act_values store q-values, by default list of lists, that is why 0 indexing\n",
    "        act_values = self.model.predict(state ,verbose=0)\n",
    "        # print(\"act_values\", act_values)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size, C=10):\n",
    "        # randomly sample some of our memory at the amount of batch_size\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        episode = 1\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward # this is the case we are done, so collect reward\n",
    "            \n",
    "            if not done:\n",
    "                if self.DDQN: \n",
    "                    # if we are not done, use NN to predict future reward\n",
    "                    # DDQN update\n",
    "                    target = (reward + self.gamma*self.model_t.predict(next_state,verbose=0)[0][np.argmax(self.model.predict(next_state,verbose=0)[0])])\n",
    "                else:\n",
    "                    # DQN & Dueling DDQN update\n",
    "                    target = (reward + self.gamma*np.amax(self.model_t.predict(next_state,verbose=0)[0]))\n",
    "            target_f = self.model.predict(state,verbose=0) # our estimated reward at current state\n",
    "            target_f[0][action] = target # map the value of current state to future state\n",
    "            \n",
    "            # single epoch of training with x=state, y=target_f (i.e. predicted future reward)\n",
    "            # 1 epoch because we only have one single memory to replay here\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            if episode % C == 0:\n",
    "                self.model_t.set_weights(self.model.get_weights()) \n",
    "            episode += 1\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a98874-95b2-46d6-894a-d2ee5a7da9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(gAgent, gbatch_size = 500, gn_episodes = 1001):\n",
    "    done = False # default state is episode not ended\n",
    "\n",
    "    for e in range(gn_episodes):\n",
    "\n",
    "        # random starting position at each episode\n",
    "        state = env.reset()\n",
    "\n",
    "        # rehape/transpose state vector to fit in the NN\n",
    "        state = np.reshape(state, [1,state_size])\n",
    "        # print(state)\n",
    "        episode_reward = 0\n",
    "        # time represents frame of the game; goal is to keep the pole upright as long as possible\n",
    "        for time in range(5000): \n",
    "            # env.render()\n",
    "\n",
    "            action = gAgent.act(state)\n",
    "\n",
    "            # print(\"time: %s action: %s\" %(time, action))\n",
    "            next_state, reward, terminated,  log = env.step(action) # agent interacts with env, gets feedback; 4 state data points\n",
    "            done = terminated\n",
    "            reward = reward \n",
    "            episode_reward += reward\n",
    "            next_state =  np.reshape(next_state, [1,state_size])\n",
    "            gAgent.remember(state, action, reward, next_state, done) # remember previous time steps state, actions, reward\n",
    "            #print(log,reward)\n",
    "            state = next_state\n",
    "\n",
    "            if done: # episode ends if agent drops pole or we reach time step 5000\n",
    "                # print the episodes score and agent's epsilon\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, gn_episodes, episode_reward, gAgent.epsilon))\n",
    "                break\n",
    "        \n",
    "        # update our \\theta (NN model params) at the end of episode\n",
    "        if len(gAgent.memory) > gbatch_size:\n",
    "            # train the agent by replaying the experiences of the episode\n",
    "            gAgent.replay(gbatch_size,20) \n",
    "\n",
    "        # every 50th episode, save out model parameters\n",
    "        # if you did not like model weights at the end of final episode, but liked something in the middle episodes, \n",
    "        # you can use those ones\n",
    "        if e % 10 == 0:\n",
    "            gAgent.save(\"model_output/weights_\" + '{:04d}'.format(e) + \".hd5\")\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0a009d-02d9-4b25-b940-4da63fb115c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_results(gAgent, gn_test_episodes = 10):\n",
    "\n",
    "    reward = 0\n",
    "    all_reward = []\n",
    "    done = False # default state is episode not ended\n",
    "    for e in range(gn_test_episodes):\n",
    "\n",
    "        # random starting position at each episode\n",
    "        state = env_test.reset()\n",
    "        state = np.reshape(state, [1,state_size])\n",
    "\n",
    "        episode_reward = []\n",
    "        acc_reward = 0\n",
    "        inv = []\n",
    "        actions = []\n",
    "        for time in range(5000): \n",
    "            # env.render()\n",
    "\n",
    "            act_values = gAgent.model.predict(state,verbose=0)\n",
    "            # print(\"act_values\", act_values)\n",
    "            action = np.argmax(act_values[0])\n",
    "            # print(\"time: %s action: %s\" %(time, action))\n",
    "            next_state, reward, terminated,  log = env_test.step(action) # agent interacts with env, gets feedback; 4 state data points\n",
    "            print(log)\n",
    "            done = terminated\n",
    "            reward = reward # reward +1 for each additional frame with pole upright\n",
    "            inv += [log['Inventory']]\n",
    "            acc_reward += reward\n",
    "            episode_reward += [acc_reward]\n",
    "            actions += [action]\n",
    "            next_state =  np.reshape(next_state, [1,state_size])\n",
    "            state = next_state\n",
    "\n",
    "            if done: # episode ends if agent drops pole or we reach time step 5000\n",
    "                # print the episodes score and agent's epsilon\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, gn_test_episodes, episode_reward, gAgent.epsilon))\n",
    "                break\n",
    "\n",
    "\n",
    "    return episode_reward,inv,actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a934c1-566e-427d-8caf-ec4ac492e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_DDQN_grocery(num_training_episodes, num_test_episodes):\n",
    "    # num of games (n_episodes) played provides data for training deep RL agent\n",
    "    # in each episode we remember some events happened in the episode, and use that memory for training purposes\n",
    "    agent = DQNAgent(state_size, action_size,False,True)\n",
    "    batch_size = 64 # hyperparameter for SGD, tunable\n",
    "    last_training_reward = train_agent(agent, gbatch_size = batch_size, gn_episodes = num_training_episodes)\n",
    "    all_test_reward = get_testing_results(agent, gn_test_episodes = num_test_episodes)\n",
    "    plt.plot(all_test_reward)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.savefig(\"Grocery_DDQN.png\")\n",
    "    avg_test_reward = sum(all_test_reward)/num_test_episodes\n",
    "    return last_training_reward, avg_test_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d98ee-4675-482b-8c7a-8623f49c2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_DQN_grocery(num_training_episodes, num_test_episodes):\n",
    "    # num of games (n_episodes) played provides data for training deep RL agent\n",
    "    # in each episode we remember some events happened in the episode, and use that memory for training purposes\n",
    "    agent = DQNAgent(state_size, action_size,False,False)\n",
    "    agent.load(\"model_output/weights_\" + '{:04d}'.format(90) + \".hd5\")\n",
    "    batch_size = 64 # hyperparameter for SGD, tunable\n",
    "    last_training_reward = train_agent(agent, gbatch_size = batch_size, gn_episodes = num_training_episodes)\n",
    "    all_test_reward = get_testing_results(agent, gn_test_episodes = num_test_episodes)\n",
    "    plt.plot(all_test_reward)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.savefig(\"Grocery_DDQN.png\")\n",
    "    avg_test_reward = sum(all_test_reward)/num_test_episodes\n",
    "    return last_training_reward, avg_test_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
